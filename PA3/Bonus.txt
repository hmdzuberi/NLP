While TF-IDF is often a very effective feature representation for text classification, it's generally not the standard or best-suited input for Naive Bayes classifiers due to the mismatch in data assumptions (counts/binary vs. real-valued weights).
You would typically expect the binary representation (or frequency counts) to work more naturally and potentially perform better with Naive Bayes because it aligns with the underlying probabilistic models (Bernoulli/Multinomial).
TF-IDF often yields better results when paired with classifiers that handle continuous, real-valued inputs well, such as Support Vector Machines (SVM) or Logistic Regression.
Therefore, switching from binary to TF-IDF for this specific Naive Bayes implementation would likely not improve accuracy and could potentially decrease it, primarily because it violates the core assumptions and mechanisms of the standard Naive Bayes algorithms. You would need to use a different classification algorithm (like SVM) to typically see the benefits of TF-IDF over simpler representations.